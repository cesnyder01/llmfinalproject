{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSLhSruyMOSYwxO+g+4yuo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesnyder01/llmfinalproject/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline Output** (untrained model)\n"
      ],
      "metadata": {
        "id": "09dMTNQVDNUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "\n",
        "# method to clean up output\n",
        "import re\n",
        "\n",
        "def trim_to_n_sentences(text, n=3):\n",
        "    # Use regex to find sentence-ending punctuation\n",
        "    sentence_endings = re.finditer(r'([.!?])\\s+', text)\n",
        "\n",
        "    count = 0\n",
        "    end_index = len(text)  # fallback in case there are fewer than n sentences\n",
        "\n",
        "    for match in sentence_endings:\n",
        "        count += 1\n",
        "        if count == n:\n",
        "            end_index = match.end()\n",
        "            break\n",
        "\n",
        "    return text[:end_index].strip()\n",
        "\n",
        "\n",
        "#\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "modelB = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # avoid padding issues\n",
        "modelB.eval()  # set model to evaluation mode\n",
        "\n",
        "# input prompts / data\n",
        "\n",
        "prompts = [\n",
        "    \"There once was a boy who went on an adventure\",\n",
        "    \"Once upon a time in a haunted forest,\",\n",
        "    \"A long time ago, in a kingdom far, far away, there lived a princess\",\n",
        "    \"Our story starts with three little kittens\",\n",
        "    \"It was a dark and stormy night\"\n",
        "]\n",
        "\n",
        "# actual generation\n",
        "\n",
        "import torch\n",
        "\n",
        "def generate_baseline(prompt, max_new_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = modelB.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,         # Random sampling makes output more natural\n",
        "            top_k=50,               # Limits to top 50 likely tokens\n",
        "            top_p=0.95,             # Nucleus sampling\n",
        "            temperature=0.9,        # Controls randomness\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# printing output\n",
        "\n",
        "for prompt in prompts:\n",
        "    generated = generate_baseline(prompt)\n",
        "    trimmed_output = trim_to_n_sentences(generated, n=5)  # get first 5 sentences\n",
        "    print(f\"\\nPrompt:\\n{prompt}\\n\\nCompletion:\\n{trimmed_output}\\n\\n{'-'*40}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agcH8dcgDLJ9",
        "outputId": "d6630258-9dfd-4f5e-83c7-0a06f5199ecb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt:\n",
            "There once was a boy who went on an adventure\n",
            "\n",
            "Completion:\n",
            "There once was a boy who went on an adventure and came back for another. He got a glimpse of himself from a place he never thought he would see again: a place where he had not felt himself since childhood. He was, in his own way, a kind of self-made man; he had seen others before him and seen a way and a way again; and yet he was a stranger, a foreigner, a little stranger, who could not go on in the same way. He was alone. He was without hope.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:\n",
            "Once upon a time in a haunted forest,\n",
            "\n",
            "Completion:\n",
            "Once upon a time in a haunted forest, as you explore a cave deep within the forest, you will see a ghostly figure of your hero.\n",
            "\n",
            "The legend of Dr. Doom has been an ongoing source of intrigue for countless years. In recent years, the world has witnessed one of these things, and is beginning to wonder if his true nature can be revealed.\n",
            "\n",
            "This is the first adventure in what has been described as a long time in the series, as we get to see the legendary Doctor fighting against his former allies.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:\n",
            "A long time ago, in a kingdom far, far away, there lived a princess\n",
            "\n",
            "Completion:\n",
            "A long time ago, in a kingdom far, far away, there lived a princess whose whole being could not comprehend the powers of men. The best she could do was to take a deep breath, and exhale it at once. In her heart she was the Goddess of All Things. I was the goddess of death. The life of the world would end in death.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:\n",
            "Our story starts with three little kittens\n",
            "\n",
            "Completion:\n",
            "Our story starts with three little kittens, named B.O.T., who are little at heart and very sad. But when a doctor says he's not having any problems, B.O.T. finally gets some rest.\n",
            "\n",
            "\"We're so grateful,\" she says. \"It's really cool.\"\n",
            "\n",
            "B.O.T.\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "Prompt:\n",
            "It was a dark and stormy night\n",
            "\n",
            "Completion:\n",
            "It was a dark and stormy night,\" he said. \"There were a lot of people. I heard people crying and people crying and people crying. People were trying to find the kids.\"\n",
            "\n",
            "The two men, the youngest of whom was about 13 and the eldest of whom was 13, were in the car with the other two at the time of the shooting, and both had their hands up in the air, according to the prosecutor.\n",
            "\n",
            "There were no injuries and no one was injured, prosecutors said.\n",
            "\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Training The Model**\n",
        "Code generated by ChatGPT using the Hugging Face transformers library"
      ],
      "metadata": {
        "id": "Q8RG3IlAtfSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hf_xet\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 needs this manually\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 2. Example dataset\n",
        "\n",
        "big_dataset = load_dataset(\"ajibawa-2023/Children-Stories-Collection\", split=\"train\")\n",
        "dataset = big_dataset.select(range(500))\n",
        "\n",
        "# 3. Tokenization\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize)\n",
        "\n",
        "# 4. Data collator (helps batch samples for language modeling)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# 5. Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,  # Small for laptops\n",
        "    save_steps=1000,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps=1000,\n",
        "    fp16=False,  # Set to True if your GPU supports it\n",
        ")\n",
        "\n",
        "# 6. Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# 7. Train!\n",
        "trainer.train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "yzVl1r8DtfZt",
        "outputId": "e4768c5b-a532-4870-8f3b-b9673d9ee43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-d0b6a9f6c7ff>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcesnyder01\u001b[0m (\u001b[33mcesnyder01-william-mary\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_204205-z0pknxk8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cesnyder01-william-mary/huggingface/runs/z0pknxk8' target=\"_blank\">./gpt2-finetuned</a></strong> to <a href='https://wandb.ai/cesnyder01-william-mary/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cesnyder01-william-mary/huggingface' target=\"_blank\">https://wandb.ai/cesnyder01-william-mary/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cesnyder01-william-mary/huggingface/runs/z0pknxk8' target=\"_blank\">https://wandb.ai/cesnyder01-william-mary/huggingface/runs/z0pknxk8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='259' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [259/500 11:25 < 10:43, 0.37 it/s, Epoch 0.52/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Rerunning The Baseline Evaluation** (trained model)"
      ],
      "metadata": {
        "id": "e8C-WvgtDfkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# generation\n",
        "\n",
        "def generate_baseline(prompt, max_new_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,         # Random sampling makes output more natural\n",
        "            top_k=50,               # Limits to top 50 likely tokens\n",
        "            top_p=0.95,             # Nucleus sampling\n",
        "            temperature=0.9,        # Controls randomness\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# printing output\n",
        "\n",
        "for prompt in prompts:\n",
        "    generated = generate_baseline(prompt)\n",
        "    trimmed_output = trim_to_n_sentences(generated, n=5)  # get first 5 sentences\n",
        "    print(f\"Prompt:\\n{prompt}\\n\\nCompletion:\\n{trimmed_output}\\n{'-'*40}\")"
      ],
      "metadata": {
        "id": "KkMjycRHDl3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}